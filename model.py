# -*- coding: utf-8 -*-
"""ISY503 - Assessment 3 (Computer Vision).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tcw6vkwvnkPCiZ0Fez2rJDkdzIu5oQy5

# [Assessment 3: Computer Vision project](https://mylearn.torrens.edu.au/courses/3784/assignments/49822)

## Setup
Include the necessary packages to import and install.
"""

!pip install patool
!pip install split-folders
!pip install pyyaml h5py

# TensorFlow and tf.keras
import tensorflow as tf

# Helper libraries
import matplotlib.pyplot as plt
import numpy as np
import os
import pathlib
import patoolib
import splitfolders

from google.colab import files
from shutil import rmtree

"""## Dataset

Constant values defined to be used all throughout the process.
"""

## Constants

# Shape of the image items
IMG_SHAPE = (66, 200)
# Shape of input for model
INPUT_SHAPE = (66, 200, 3)
# Exported model filename
MODEL_FILENAME = "model.h5"
# Used for shuffling and transformation of values
SEED = 123

"""Helper functions added to prepare the dataset, from extraction, to splitting of the folders."""

## Helper functions

def get_labels(ds):
    """Get labels for each dataset rows."""
    return np.hstack([y for x, y in ds])

def load_image_dataset(dir, subset=None, shuffle=True):
    """Load dataset from directory using the
    `image_dataset_from_directory` method."""
    if not subset:
        return tf.keras.utils.image_dataset_from_directory(
            dir,
            shuffle=shuffle,
            seed=SEED,
            image_size=IMG_SHAPE
        )

    return tf.keras.utils.image_dataset_from_directory(
        dir,
        subset=subset,
        validation_split=0.2,
        shuffle=shuffle,
        seed=SEED,
        image_size=IMG_SHAPE
    )

def get_datasets_from_session(path):
    """Get the training, validation, and testing datasets from
    a storage session path in Colab."""
    base_dir = "dataset"

    # Check if output directory already exists; Remove to ensure clean directory
    if os.path.exists(base_dir):
        rmtree(base_dir)

    # Split folders for easier division of dataset into training,
    # validation, and testing
    splitfolders.ratio(
        path,
        output=base_dir,
        seed=SEED,
        # ratio=(.7, .18, .12)
        ratio=(.8, .1, .1)
    )

    # Load dataset from Colab session storage directory
    train_ds = load_image_dataset(f"{base_dir}/train")
    val_ds = load_image_dataset(f"{base_dir}/val")
    test_ds = load_image_dataset(f"{base_dir}/test")

    # Remove unsegmented dataset directory
    rmtree(path)

    return (train_ds, val_ds, test_ds)

def get_datasets_from_upload(filename=None):
    """Get the training, validation, and testing datasets
    from uploading a local folder to Colab storage."""
    data_dir = "/content/temp"

    # Launch option to upload from local
    if not filename:
        upload = files.upload()
        filename = next(iter(upload.keys()))

    path = f"/content/{filename}"

    # Check if output directory already exists; Remove to allow extraction
    if os.path.exists(data_dir):
        rmtree(data_dir)

    patoolib.extract_archive(path, verbosity=-1, outdir=data_dir)

    # Remove uploaded file after extraction
    os.remove(filename)

    return get_datasets_from_session(data_dir)

"""### Fetch Dataset
For this test, the dataset is expected to be uploaded locally. It needs to be in a compressed format (e.g. .zip) in order for the script to proceed without any issues. Extraction and the splitting of data into training, validation, and testing subsets are then performed afterwards.
"""

## Helper functions

def count_unique_values(array):
    """Count occurence of each value from provided array.
    Useful for verifying distribution of classes for each dataset."""
    # Get unique values and counts of each value
    unique, counts = np.unique(array, return_counts=True)

    # Display unique values and counts side by side
    return np.asarray((unique, counts)).T

def display_labels_distribution(train_ds, val_ds, test_ds):
    """Display the number of occurence for each label for
    training, testing, and validation datasets."""
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)

    fig.tight_layout(pad=1.0)

    # Display Training plot
    unique, counts = np.unique(get_labels(train_ds), return_counts=True)

    ax1.bar(CLASS_NAMES, counts)
    ax1.set_title("Training")

    # Display Validation plot
    unique, counts = np.unique(get_labels(val_ds), return_counts=True)

    ax2.bar(CLASS_NAMES, counts)
    ax2.set_title("Validation")

    # Display Testing plot
    unique, counts = np.unique(get_labels(test_ds), return_counts=True)

    ax3.bar(CLASS_NAMES, counts)
    ax3.set_title("Testing")

train_ds, val_ds, test_ds = get_datasets_from_upload()

CLASS_NAMES = train_ds.class_names
NUM_CLASSES = len(CLASS_NAMES)

# Check distribution of each label at each dataset
display_labels_distribution(train_ds, val_ds, test_ds)

"""### Optimise Dataset for Performance
Buffered prefetching is applied for improved performance during training. More details can be found [here](https://www.tensorflow.org/tutorials/load_data/images#configure_the_dataset_for_performance) and [here](https://www.tensorflow.org/tutorials/load_data/images#configure_dataset_for_performance).
"""

## Helper functions

def configure_for_performance(ds):
    """Configure passed dataset to optimise performance."""
    ds = ds.cache()
    ds = ds.shuffle(buffer_size=1000)
    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)

    return ds

train_ds = configure_for_performance(train_ds)
val_ds = configure_for_performance(val_ds)
test_ds = configure_for_performance(test_ds)

"""### Visualise Data
Here, the first nine items of our training dataset are displayed for verification.
"""

## Helper functions

def first_n(ds, num=9):
    """Display first n images for a given dataset. `n` defaults to `9`."""
    plt.figure(figsize=(10, 10))

    for images, labels in ds.take(1):
        num_images = images.shape[0]
        num = min(num, num_images)

        for i in range(num):
            plt.subplot(3, 3, i + 1)
            plt.imshow(images[i].numpy().astype("uint8"))
            plt.title(CLASS_NAMES[labels[i]])
            plt.axis("off")


first_n(train_ds)

"""### Preprocess Data
Here, values are standardised to be in the `[0, 1]` range to fit better in a neural network.
"""

normalization_layer = tf.keras.layers.Rescaling(1./255)

train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))
test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))

"""## Model
The model mainly consists of 5 convolution blocks. Based on the [NVIDIA model](https://github.com/naokishibuya/car-behavioral-cloning).
"""

def nvidia_model():
    """Model created based on NVIDIA model as stated in their
    End-to-End Deep Learning for Self-Driving Cars article.
    See https://developer.nvidia.com/blog/deep-learning-self-driving-cars/."""
    model = tf.keras.Sequential()

    # Normalise input images
    model.add(tf.keras.layers.Lambda(lambda x: x / 127.5 - 1.0, input_shape=INPUT_SHAPE))

    # Convolutional layer
    model.add(tf.keras.layers.Conv2D(24, 5, activation='elu', strides=(2, 2)))
    model.add(tf.keras.layers.Conv2D(36, 5, activation='elu', strides=(2, 2)))
    model.add(tf.keras.layers.Conv2D(48, 5, activation='elu', strides=(2, 2)))
    model.add(tf.keras.layers.Conv2D(64, 3, activation='elu'))
    model.add(tf.keras.layers.Conv2D(64, 3, activation='elu'))
    model.add(tf.keras.layers.Dropout(0.3))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(100, activation='elu'))
    model.add(tf.keras.layers.Dense(50, activation='elu'))
    model.add(tf.keras.layers.Dense(10, activation='elu'))
    model.add(tf.keras.layers.Dense(1))

    return model

# Remove exported model in preparation of saving new instance
if os.path.exists(MODEL_FILENAME):
    os.remove(MODEL_FILENAME)

model = nvidia_model()

"""### Compilation
Uses the `Adam` optimiser and the `mean_squared_error` loss function. Metrics is set to `accuracy` to view its training and validation accuracy for each epoch pass.
"""

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    # optimizer=tf.keras.optimizers.Adam(learning_rate=1.0e-4),
    loss="mean_squared_error",
    metrics=["accuracy"]
)

"""### Training
Model is now trained. Its history will be stored for plotting. Added an `EarlyStopping` callback to prevent overfitting.

"""

# Number of epochs for training
EPOCHS = 100

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=[callback]
)

"""#### Visualise Results
Based on the model's training history, we can plot graphs for accuracy and loss to better illustrate and understand the model's overall performance.
"""

## Helper functions

def visualise_history(history, epochs):
    """Plot out statistics of model such as accuracy and other metrics to a graph."""
    acc = history.history["accuracy"]
    val_acc = history.history["val_accuracy"]

    loss = history.history["loss"]
    val_loss = history.history["val_loss"]
    num_epochs = len(loss)

    epochs_range = range(min(num_epochs, epochs))

    plt.figure(figsize=(8, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label="Training Accuracy")
    plt.plot(epochs_range, val_acc, label="Validation Accuracy")
    plt.legend(loc="upper right")
    plt.title("Accuracy")

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label="Training Loss")
    plt.plot(epochs_range, val_loss, label="Validation Loss")
    plt.legend(loc="upper right")
    plt.title("Loss")
    plt.show()


visualise_history(history, EPOCHS)

"""#### Evaluation
Model is then evaluated using the testing dataset. Accuracy is displayed for initial evaluation.
"""

test_loss, test_acc = model.evaluate(test_ds, verbose=2)

print(f"\nTest accuracy: {test_acc:.1%}")

"""## Predictions
To better evaluate the performance of the model, various helper methods were created to plot various elements, such as the image being tested, the graph for its predictions, outlining its predicted, and actual values, and whether the model got the correct prediction, among others.
"""

## Helper functions for extracting details and predictions.

def get_prediction_details(predictions):
    """Get corresponding label and percentage of predicted class."""
    label_index = np.argmax(predictions)
    percentage = 100 * np.max(predictions)

    return (label_index, percentage)

## Helper functions for plotting graphs.

def plot_prediction(i, predictions, sample):
    """Helper function to plot a feature and the labels to a graph with its corresponding prediction values."""
    prediction_details = get_prediction_details(predictions[i])

    plt.figure(figsize=(6, 3))
    plt.subplot(1, 2, 1)
    plot_image(sample, prediction_details)
    plt.show()

def plot_image(sample, prediction_details):
    """Plot image to visualise the feature."""
    img, actual_label = sample

    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img, cmap=plt.cm.binary)

    predicted_label, percentage = prediction_details
    color = "blue" if predicted_label == actual_label else "red"
    label = "Predicted: {}\nActual: {}\nPercentage: {:2.0f}%"

    plt.xlabel(label.format(CLASS_NAMES[predicted_label],
                            CLASS_NAMES[actual_label],
                            percentage),
               color=color)

def plot_value_array(i, predictions, sample):
    """Plot feature classes and the value of predictions to visualise result."""
    _, actual_label = sample

    plt.grid(False)
    plt.xticks(range(NUM_CLASSES), CLASS_NAMES, rotation=45)
    plt.yticks([])

    predicted_label, _ = get_prediction_details(predictions[i])
    thisplot = plt.bar(range(NUM_CLASSES), predictions[i], color="#777777")

    plt.ylim([0, 1])

    thisplot[predicted_label].set_color("red")
    thisplot[actual_label].set_color("blue")

"""### Probability Model
Create a probability model by attaching a softmax layer for easier interpretation. More details can be found [here](https://www.tensorflow.org/tutorials/keras/classification#make_predictions).
"""

MAX_LENGTH = 5

# Model for predictions
probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])
predictions = probability_model.predict(test_ds)

# Configuration for plotting set number of features
predictions_len = len(predictions)
length = predictions_len if predictions_len <= MAX_LENGTH else MAX_LENGTH

# Extract feature and labels
[(features, label_batch)] = test_ds.take(1)
features = np.array(features)
labels = np.array(label_batch)

# Display predictions per feature
for i in range(0, length):
    plot_prediction(i, predictions, (features[i], labels[i]))

"""## Save Model

Save model in preparation for loading into the Car Driving simulator. See [link](https://www.tensorflow.org/tutorials/keras/save_and_load) for details on saving and loading models.
"""

model.save("model.h5")